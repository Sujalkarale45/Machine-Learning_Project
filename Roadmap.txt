Data - > Web Scrapping - > Preprocessing - > Model - > Website - > Deploy


Web Scrapping : 
Using BeautifulSoup Python Library 
extracting Youtube by Youtube API 
Creating a dataframe and Fetch data in it

For an **Education-Based Recommendation System** using YouTube API, you need to fetch relevant metadata about videos that can help in making recommendations based on user interests.  

---

## **1. Fetching Data from YouTube API**
You can use **YouTube Data API v3** to fetch data. The main API endpoints you'll use:  
- `search` (to find educational videos based on keywords)
- `videos` (to get detailed information about videos)
- `channels` (to get details about the creator)

---

## **2. Recommended Dataframe Columns**
Here’s a structured **DataFrame schema** for storing the video data:

| Column Name             | Data Type | Description |
|-------------------------|----------|-------------|
| `video_id`             | String   | Unique identifier for the video |
| `title`                | String   | Video title |
| `channel_id`          | String   | Unique ID of the channel |
| `channel_name`        | String   | Name of the channel |
| `category_id`        | Integer  | ID representing the video category (Education, Science, etc.) |
| `tags`                | List     | Keywords associated with the video |
| `description`         | String   | Video description (optional) |
| `publish_date`        | DateTime | Date when the video was published |
| `duration`            | Integer  | Video length in seconds |
| `views`              | Integer  | Number of views |
| `likes`              | Integer  | Number of likes |
| `dislikes`          | Integer  | Number of dislikes (deprecated by YouTube) |
| `comments_count`     | Integer  | Number of comments |
| `engagement_score`   | Float    | Custom metric = (likes + comments) / views |
| `thumbnail_url`      | String   | URL of the video thumbnail |
| `video_url`          | String   | Direct URL to the video |
| `region`            | String   | Country/Region of the video (if available) |

---

## **3. Important Features for Recommendations**
Since you are **not using NLP**, your ML model will rely on numerical and categorical features:
- **Numerical Features:** `views`, `likes`, `comments_count`, `duration`, `engagement_score`
- **Categorical Features:** `category_id`, `channel_name`
- **Boolean Features (One-Hot Encoding):** `tags`
- **Time-Based Features:** `publish_date` (to recommend recent content)

---

## **4. Steps to Fetch Data using YouTube API**
1. **Get an API Key** from [Google Developer Console](https://console.cloud.google.com/)
2. **Use `googleapiclient` library in Python**
3. **Call `search` API** to get video IDs for educational topics
4. **Call `videos` API** to get detailed metadata
5. **Store data in a Pandas DataFrame**

Preprocessing : 
### **Data Preprocessing for Education-Based Recommendation System**  

Since you are not using NLP, the preprocessing will focus on **handling missing values, feature engineering, and encoding categorical variables** to ensure the dataset is ready for machine learning.

---

## **1. Handling Missing or Incomplete Data**
- Some videos might not have likes, comments, or category IDs.
- **Solution:**  
  - Fill missing numerical values with the **median** (e.g., missing `views` or `likes`).
  - Fill missing categorical values with **"Unknown"** or the most frequent value.

---

## **2. Converting Data Types**
- Convert `publish_date` to **datetime format** to extract useful features (e.g., `year`, `month`).
- Convert `duration` (ISO 8601 format) into **seconds** for numerical processing.
- Convert `category_id` to **integer** if not already.

---

## **3. Feature Engineering**
Creating new features that improve recommendations:

| Feature Name          | Type   | Description |
|----------------------|--------|-------------|
| `engagement_score`  | Float  | `(likes + comments) / views` (measures interaction level) |
| `is_popular`        | Bool   | 1 if `views` > threshold (e.g., 100k), else 0 |
| `recent_video`      | Bool   | 1 if published in the last 3 months, else 0 |
| `watch_time_score`  | Float  | `views * duration` (longer videos with more views score higher) |
| `publish_hour`      | Int    | Extracted from `publish_date` (useful for trending analysis) |
| `day_of_week`       | Int    | Extracted from `publish_date` |

---

## **4. Encoding Categorical Variables**
- **One-Hot Encoding** for `category_id` and `region` (if limited categories).
- **Label Encoding** for `channel_name` to convert it into numerical values.

---

## **5. Normalization & Scaling**
- **Scale numerical features** (`views`, `likes`, `comments_count`, `duration`, `engagement_score`) using **Min-Max Scaling** or **Standardization** to improve ML model performance.
  
  ```python
  from sklearn.preprocessing import MinMaxScaler

  scaler = MinMaxScaler()
  df[['views', 'likes', 'comments_count', 'duration', 'engagement_score']] = scaler.fit_transform(
      df[['views', 'likes', 'comments_count', 'duration', 'engagement_score']]
  )
  ```

---

## **6. Removing Outliers**
- Remove **extremely high or low values** in `views`, `likes`, and `comments_count` using **IQR (Interquartile Range)** method.
  
  ```python
  import numpy as np

  Q1 = df['views'].quantile(0.25)
  Q3 = df['views'].quantile(0.75)
  IQR = Q3 - Q1

  df = df[(df['views'] > (Q1 - 1.5 * IQR)) & (df['views'] < (Q3 + 1.5 * IQR))]
  ```

---

### **Final Steps**
✅ Handle missing values  
✅ Convert `publish_date` and `duration` into numeric features  
✅ Create new features (`engagement_score`, `watch_time_score`)  
✅ Encode categorical variables  
✅ Normalize numerical features  
✅ Remove outliers  

---

Once preprocessing is done, the dataset will be **ready for machine learning models** like Content-Based Filtering or Collaborative Filtering.

Model : 
### **Building the ML Model for the Education Recommendation System**
Since you are not using NLP, the recommendation system will rely on **numerical and categorical features**.  
There are **two main approaches** for recommendation:  

1. **Content-Based Filtering** (based on video metadata)
2. **Collaborative Filtering** (based on user interactions)

---

## **1. Content-Based Filtering (CBF)**
This method recommends videos **similar to a user's watched video** based on features like views, likes, duration, engagement, etc.

### **Steps to Build Content-Based Filtering**
📌 **Step 1: Select Features for Similarity Calculation**  
- `category_id`
- `views`
- `likes`
- `comments_count`
- `engagement_score`
- `duration`
- `watch_time_score`

📌 **Step 2: Normalize the Features**  
Since features like `views` and `likes` have different scales, use `MinMaxScaler`.

```python
from sklearn.preprocessing import MinMaxScaler
import pandas as pd

scaler = MinMaxScaler()
features = ['category_id', 'views', 'likes', 'comments_count', 'engagement_score', 'duration']
df[features] = scaler.fit_transform(df[features])
```

📌 **Step 3: Compute Similarity (Cosine Similarity)**  
- Calculate **pairwise similarity** between videos.
- Use **Cosine Similarity** from `sklearn.metrics.pairwise`.

```python
from sklearn.metrics.pairwise import cosine_similarity

# Compute similarity matrix
similarity_matrix = cosine_similarity(df[features])

# Convert it into a DataFrame
similarity_df = pd.DataFrame(similarity_matrix, index=df['video_id'], columns=df['video_id'])
```

📌 **Step 4: Recommend Videos Based on Similarity**  
- When a user watches a video, recommend **top N similar videos**.

```python
def recommend_videos(video_id, num_recommendations=5):
    # Get similarity scores for the given video
    similar_videos = similarity_df[video_id].sort_values(ascending=False)
    
    # Return top N similar videos
    return similar_videos.iloc[1:num_recommendations+1].index.tolist()

# Example usage
recommend_videos('abcd1234', num_recommendations=5)
```

📌 **Step 5: Convert It into a Function & API**  
- Deploy this model using **Flask or FastAPI** to serve recommendations.

---

## **2. Collaborative Filtering (CF)**
If you have user interaction data (e.g., which videos they watched, liked), you can use **Collaborative Filtering**.

### **Types of CF**
- **User-User CF** → Recommends videos watched by similar users.
- **Item-Item CF** → Recommends videos similar to previously watched videos.

### **Steps to Build Collaborative Filtering**
📌 **Step 1: Create a User-Video Interaction Matrix**  
Convert data into a **matrix where rows = users, columns = videos**.

```python
from surprise import Dataset, Reader
import pandas as pd

# Load user interaction data (user_id, video_id, watch_time)
df_ratings = pd.read_csv('user_video_data.csv')

reader = Reader(rating_scale=(0, 1))  # Normalize watch time between 0 and 1
data = Dataset.load_from_df(df_ratings[['user_id', 'video_id', 'watch_time']], reader)
```

📌 **Step 2: Train a CF Model using SVD**  
Singular Value Decomposition (SVD) is a popular algorithm for CF.

```python
from surprise import SVD
from surprise.model_selection import train_test_split
from surprise import accuracy

trainset, testset = train_test_split(data, test_size=0.2)
model = SVD()
model.fit(trainset)

predictions = model.test(testset)
print("RMSE:", accuracy.rmse(predictions))
```

📌 **Step 3: Make Video Recommendations for a User**  
- **Predict missing ratings** for videos the user has not watched.
- Recommend **top N highest-rated videos**.

```python
def recommend_for_user(user_id, num_recommendations=5):
    video_ids = df['video_id'].unique()
    predictions = [model.predict(user_id, vid) for vid in video_ids]
    
    # Sort by predicted rating
    recommendations = sorted(predictions, key=lambda x: x.est, reverse=True)[:num_recommendations]
    
    return [pred.iid for pred in recommendations]

# Example usage
recommend_for_user(1234, num_recommendations=5)
```

Website : 
### **Turning the Recommendation Model into a Website**  

To convert the **Education Recommendation System** into a fully functional **website**, follow these steps:

---

## **1. Tech Stack for Web Application**
🔹 **Backend** → `Flask` or `FastAPI` (for API)  
🔹 **Frontend** → `HTML`, `CSS`, `Bootstrap`, `JavaScript`  
🔹 **Database** → `SQLite` / `MongoDB` (to store scraped data)  
🔹 **Machine Learning** → `Scikit-learn` / `Surprise` (for recommendations)  
🔹 **Deployment** → `Docker + AWS/GCP`

---

## **2. Steps to Build the Website**
1️⃣ **Scrape YouTube Data** → Store in database  
2️⃣ **Train ML Model** → Content-Based & Collaborative Filtering  
3️⃣ **Create Flask API** → Serve recommendations  
4️⃣ **Build Web Interface** → Users can search and get recommended videos  
5️⃣ **Deploy** → Host on AWS/GCP  

---

## **3. Backend (Flask API for Recommendations)**
🔹 **Install Flask**  
```bash
pip install flask
```

🔹 **Create `app.py` (Flask Backend)**  
```python
from flask import Flask, request, jsonify
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from surprise import SVD, Dataset, Reader

app = Flask(__name__)

# Load preprocessed video dataset
df = pd.read_csv('youtube_data.csv')

# Content-Based Filtering Model (Cosine Similarity)
def recommend_videos(video_id, num_recommendations=5):
    features = ['category_id', 'views', 'likes', 'comments_count', 'engagement_score']
    similarity_matrix = cosine_similarity(df[features])
    
    # Convert similarity to DataFrame
    similarity_df = pd.DataFrame(similarity_matrix, index=df['video_id'], columns=df['video_id'])
    
    similar_videos = similarity_df[video_id].sort_values(ascending=False)
    return similar_videos.iloc[1:num_recommendations+1].index.tolist()

@app.route('/recommend', methods=['GET'])
def recommend():
    video_id = request.args.get('video_id')
    recommendations = recommend_videos(video_id)
    return jsonify({"recommended_videos": recommendations})

if __name__ == '__main__':
    app.run(debug=True)
```

---

## **4. Frontend (HTML + Bootstrap)**
🔹 **Create `templates/index.html`**
```html
<!DOCTYPE html>
<html lang="en">
<head>
    <title>Education Video Recommender</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css">
</head>
<body>
    <div class="container mt-5">
        <h2>Education Video Recommender</h2>
        <form id="searchForm">
            <input type="text" id="video_id" class="form-control" placeholder="Enter Video ID">
            <button type="submit" class="btn btn-primary mt-2">Get Recommendations</button>
        </form>
        <div id="results" class="mt-4"></div>
    </div>

    <script>
        document.getElementById('searchForm').addEventListener('submit', function(event) {
            event.preventDefault();
            let video_id = document.getElementById('video_id').value;
            
            fetch(`/recommend?video_id=${video_id}`)
                .then(response => response.json())
                .then(data => {
                    let resultsDiv = document.getElementById('results');
                    resultsDiv.innerHTML = "<h4>Recommended Videos:</h4>";
                    data.recommended_videos.forEach(video => {
                        resultsDiv.innerHTML += `<p>${video}</p>`;
                    });
                });
        });
    </script>
</body>
</html>
```

---

## **5. Run the Website**
- Start the Flask server  
```bash
python app.py
```
- Open `http://127.0.0.1:5000/` in your browser  

---

## **6. Deployment**
🔹 **Containerize with Docker**  
```dockerfile
# Dockerfile
FROM python:3.9
WORKDIR /app
COPY . .
RUN pip install flask pandas scikit-learn surprise
CMD ["python", "app.py"]
```
🔹 **Deploy on AWS/GCP**  
- Use **EC2 Instance** (AWS) or **Google Cloud Run**  
- Expose `Flask API` to public  

---

## **Next Steps**
✅ Scrape Data → ✅ Build ML Model → ✅ Create Flask API → ✅ Build UI → 🚀 Deploy  

Deploy : 
### **Deploy Flask App on Docker for Free 🚀**  

You can **deploy your Flask-based YouTube Recommendation System** for free using **Docker + Render/Vercel** or host it locally on a cloud VM.  

---

## **1️⃣ Install Docker Locally**
First, install Docker on your system:  
🔹 **Windows/Mac:** [Download Docker Desktop](https://www.docker.com/get-started)  
🔹 **Linux (Ubuntu):**  
```bash
sudo apt update
sudo apt install docker.io
```
🔹 **Verify Installation**  
```bash
docker --version
```

---

## **2️⃣ Create a Dockerfile**
In your project directory, create a file named **`Dockerfile`**:
```dockerfile
# Use Python 3.9 as base image
FROM python:3.9

# Set the working directory
WORKDIR /app

# Copy all files into the container
COPY . .

# Install dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Expose port 5000
EXPOSE 5000

# Run the Flask app
CMD ["python", "app.py"]
```

---

## **3️⃣ Create `requirements.txt`**
List all dependencies for Flask in **`requirements.txt`**:
```
flask
pandas
scikit-learn
surprise
gunicorn
```

---

## **4️⃣ Build and Run Docker Locally**
🔹 **Build Docker Image**  
```bash
docker build -t youtube-recommender .
```
🔹 **Run Container Locally**  
```bash
docker run -p 5000:5000 youtube-recommender
```
🔹 **Access Flask App**  
Go to **http://127.0.0.1:5000/**  

---

## **5️⃣ Deploy on Free Cloud Service**
Now, let’s deploy it for free using **Render or Railway**.

### **🔹 Option 1: Deploy on Render (FREE)**
1. Create an account on [Render](https://render.com/)  
2. Go to **Dashboard → New Web Service**  
3. Connect your **GitHub Repository**  
4. Choose **Docker** as the deployment method  
5. Set **PORT=5000** in environment variables  
6. Deploy! 🎉

---

### **🔹 Option 2: Deploy on Railway (FREE)**
1. Create an account on [Railway](https://railway.app/)  
2. Click **New Project → Deploy from GitHub**  
3. Add **Dockerfile** in your repo  
4. Set **PORT=5000** in Railway environment variables  
5. Click **Deploy**  

---

## **6️⃣ Verify Deployment**
Once deployed, your website will be available at:  
**https://your-app-name.render.com/** 🎉  

---

### **Next Steps**
✅ **Scrape YouTube Data**  
✅ **Train ML Model**  
✅ **Build Flask API**  
✅ **Dockerize App**  
🚀 **Deploy for Free**  

